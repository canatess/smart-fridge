{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e583961d-92eb-4b05-8acf-adcf58e403c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from ultralytics import YOLO\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "acbdfaeb-d030-48a7-8f5c-7c94f522546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_c)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_c)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class encoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.conv = conv_block(in_c, out_c)\n",
    "        self.pool = nn.MaxPool2d((2, 2))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        p = self.pool(x)\n",
    "        return x, p\n",
    "\n",
    "\n",
    "class decoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv = conv_block(out_c + out_c, out_c)\n",
    "\n",
    "    def forward(self, inputs, skip):\n",
    "        x = self.up(inputs)\n",
    "        # Adjust the dimensions of `skip` to match `x` before concatenation\n",
    "        skip = F.interpolate(\n",
    "            skip, size=(x.size(2), x.size(3)), mode=\"bilinear\", align_corners=True\n",
    "        )\n",
    "        x = torch.cat([x, skip], dim=1)  # Use `dim` instead of `axis`\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class build_unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \"\"\" Encoder \"\"\"\n",
    "        self.e1 = encoder_block(3, 64)\n",
    "        self.e2 = encoder_block(64, 128)\n",
    "        self.e3 = encoder_block(128, 256)\n",
    "        self.e4 = encoder_block(256, 512)\n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        self.b = conv_block(512, 1024)\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        self.d1 = decoder_block(1024, 512)\n",
    "        self.d2 = decoder_block(512, 256)\n",
    "        self.d3 = decoder_block(256, 128)\n",
    "        self.d4 = decoder_block(128, 64)\n",
    "        \"\"\" Classifier \"\"\"\n",
    "        self.outputs = nn.Conv2d(64, 1, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Encoder\"\"\"\n",
    "        s1, p1 = self.e1(inputs)\n",
    "        s2, p2 = self.e2(p1)\n",
    "        s3, p3 = self.e3(p2)\n",
    "        s4, p4 = self.e4(p3)\n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        b = self.b(p4)\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        d1 = self.d1(b, s4)\n",
    "        d2 = self.d2(d1, s3)\n",
    "        d3 = self.d3(d2, s2)\n",
    "        d4 = self.d4(d3, s1)\n",
    "        \"\"\" Classifier \"\"\"\n",
    "        outputs = self.outputs(d4)\n",
    "        return outputs\n",
    "\n",
    "# Define the model\n",
    "class RegressionModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(RegressionModel, self).__init__()\n",
    "        resnet = torchvision.models.resnet34()\n",
    "        self.features = nn.Sequential(\n",
    "            *list(resnet.children())[:-1]\n",
    "        )  # Remove the last fully connected layer\n",
    "        self.regressor = nn.Linear(\n",
    "            512, 1\n",
    "        )  # Replace the last layer with a regression layer\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.regressor(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model_reg = RegressionModel()\n",
    "\n",
    "# Load the pre-trained model\n",
    "model_seg = build_unet()\n",
    "\"\"\"\n",
    "checkpoint = torch.load(\"mold_model_comb.pth\")\n",
    "model_seg.load_state_dict(checkpoint)\n",
    "\"\"\"\n",
    "model = torch.load('regression_model.pth',map_location ='cpu')\n",
    "model_reg.eval()\n",
    "model_seg.eval()\n",
    "\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "test_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(224, 224),  # Resize images to 224x224\n",
    "        A.Normalize(mean=mean, std=std),  # Normalize images using general mean and std\n",
    "        ToTensorV2(),  # Convert image to PyTorch tensor\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Define the prediction function\n",
    "def predict_image(image):\n",
    "    # Apply transformations to the input image\n",
    "    img_numpy = test_transform(image=np.array(image))[\"image\"]\n",
    "    image_tensor = img_numpy.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Perform prediction\n",
    "    with torch.no_grad():\n",
    "        output = model_seg(image_tensor)\n",
    "\n",
    "    # Apply sigmoid activation and thresholding\n",
    "    preds = torch.sigmoid(output)\n",
    "    preds = output.detach().squeeze().numpy()\n",
    "\n",
    "    preds = np.where(preds >= 0.5, 1, 0)\n",
    "    pred_indices = np.where(preds == 1)\n",
    "\n",
    "    masked_image = torch.squeeze(image_tensor).permute(1, 2, 0).numpy()\n",
    "\n",
    "    masked_image = (masked_image * std) + mean\n",
    "\n",
    "    # Visualize the predictions\n",
    "    cmap = plt.get_cmap(\"jet\")\n",
    "    masked_image[pred_indices] = cmap(preds[pred_indices])[:, :3]\n",
    "\n",
    "    # Normalize the image array to be between -1 and 1\n",
    "    return masked_image\n",
    "\n",
    "def regression(image):\n",
    "    img_numpy = test_transform(image=np.array(image))[\"image\"]\n",
    "    image_tensor = img_numpy.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model_reg(image_tensor)\n",
    "    \n",
    "    print(output.shape)\n",
    "    return output.squeeze().tolist()\n",
    "\n",
    "def hugg_face(img):\n",
    "    model = YOLO(\"peachlemonv3.pt\")\n",
    "    labels = [\"freshpeach\", \"freshlemon\", \"rottenpeach\", \"rottenlemon\"]\n",
    "\n",
    "    img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    results = model(img)\n",
    "\n",
    "    img_label_results = []\n",
    "    img_2 = img.copy()\n",
    "    \n",
    "    for result in results:\n",
    "        for i, cls in enumerate(result.boxes.cls):\n",
    "            crop_img = img[int(result.boxes.xyxy[i][1]):int(result.boxes.xyxy[i][3]),\n",
    "                       int(result.boxes.xyxy[i][0]):int(result.boxes.xyxy[i][2])]\n",
    "            cv2.rectangle(img_2, (int(result.boxes.xyxy[i][0]), int(result.boxes.xyxy[i][1])),\n",
    "                          (int(result.boxes.xyxy[i][2]), int(result.boxes.xyxy[i][3])), (0, 255, 0), 2)\n",
    "            cv2.putText(img_2, labels[int(cls)] + str(i), (int(result.boxes.xyxy[i][0]), int(result.boxes.xyxy[i][1])),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "            img_label_results.append({\"label\": labels[int(cls)] + str(i), \"crop_img\": crop_img})\n",
    "\n",
    "    \n",
    "    img_2_pil = Image.fromarray(cv2.cvtColor(img_2, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    regression_results = []\n",
    "\n",
    "    # Iterate over cropped images and their labels\n",
    "    for item in img_label_results:\n",
    "        label = item[\"label\"]\n",
    "        cropped_img = item[\"crop_img\"]\n",
    "        \n",
    "        # Apply regression to the cropped image\n",
    "        regression_output = regression(cropped_img)\n",
    "        \n",
    "        # Append the regression output along with its label to the regression_results list\n",
    "        regression_results.append({\"label\": label, \"Rotten Proportion\": regression_output})\n",
    "\n",
    "    return img_2_pil, regression_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a469f152-90b1-460b-9617-b7e9427c11ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7885\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7885/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in callback _ProactorBasePipeTransport._call_connection_lost(None)\n",
      "handle: <Handle _ProactorBasePipeTransport._call_connection_lost(None)>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\bahak\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\bahak\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\proactor_events.py\", line 162, in _call_connection_lost\n",
      "    self._sock.shutdown(socket.SHUT_RDWR)\n",
      "ConnectionResetError: [WinError 10054] Varolan bir bağlantı uzaktaki bir ana bilgisayar tarafından zorla kapatıldı\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 13 freshlemons, 2 rottenlemons, 242.0ms\n",
      "Speed: 4.0ms preprocess, 242.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Create Gradio interface\n",
    "inputs = gr.Image(type=\"pil\")  # Define input image shape\n",
    "\n",
    "# Define output types: one for the text box (for regression results) and one for the image (segmented image)\n",
    "outputs = [gr.Image(type=\"pil\", label=\"Detecion Result\"), gr.Textbox(label=\"Regression Results\")]\n",
    "\n",
    "app = gr.Interface(\n",
    "    fn=hugg_face,\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    title=\"Image Segmentation with Regression\",\n",
    "    description=\"Segmentation of input image using a U-Net model with regression results.\"\n",
    ")\n",
    "\n",
    "# Launch the app\n",
    "app.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
